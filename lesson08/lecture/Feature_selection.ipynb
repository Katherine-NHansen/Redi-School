{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a436b149",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Selection in Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e1415",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is feature selection ?\n",
    "\n",
    "Feature selection is the procedure of selecting a subset (some out of all available) of the input variables that are most relevant to the target variable (that we wish to predict).\n",
    "\n",
    "Target variable here refers to the variable that we wish to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90c361",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The 2 most famous feature selection techniques that can be used for numerical input data and a numerical target variable are the following:\n",
    "\n",
    "1. Correlation (Pearson, spearman)\n",
    "2. Mutual Information (MI, normalized MI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e86e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlation is a measure of how two variables change together. The most widely used correlation measure is the Pearson’s correlation that assumes a Gaussian distribution of each variable and detects linear relationship between numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b859be",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is done in 2 steps:\n",
    "\n",
    "1. The correlation between each regressor and the target is computed, that is, ((X[:, i] — mean(X[:, i])) * (y — mean_y)) / (std(X[:, i]) * std(y)).\n",
    "2. It is converted to an F score then to a p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd19c9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Mutual information originates from the field of information theory. The idea is that the information gain (typically used in the construction of decision trees) is applied in order to perform the feature selection. Mutual information is calculated between two variables and measures as the reduction in uncertainty for one variable given a known value of the other variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0a922",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion : Using either the Correlation metric or the Mutual Information metric , we can easily estimate the relationship between each input variable and the target variable"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
