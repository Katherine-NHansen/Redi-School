{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework:\n",
    "\n",
    "This week, there are two homework tasks, of which the web scraping task is optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Task\n",
    "\n",
    "1. Go to https://spoonacular.com/food-api and get familiar with the information you can get from this API. Find out how to authenticate a request to this API.\n",
    "2. Retrieve 150 random recipes\n",
    "4. Create a .csv file with the following headers:\n",
    "- vegetarian\n",
    "- glutenFree\n",
    "- dairyFree\n",
    "- veryHealthy\n",
    "- healthScore\n",
    "- aggregateLikes\n",
    "- id\n",
    "- title\n",
    "- pricePerServing\n",
    "- readyInMinutes\n",
    "- servings\n",
    "- sourceUrl\n",
    "- there is a list of nutrients, create two columns per nutrient: one stating the amount and one the percentOfDailyNeeds. The name is e.g. \"seleniumAmount\" and \"seleniumPercentOfDailyNeed\"\n",
    "\n",
    "**Handin:**\n",
    "- your notebook (.ipynb file)\n",
    "- your .csv file\n",
    "\n",
    "or\n",
    "\n",
    "- link to GitHub folder containing both files\n",
    "\n",
    "**Hints:**\n",
    "- In order to retrieve the information you are looking for, you need to add a query to the request url, which is described in the API documentation\n",
    "- Please be aware, that the use of this API is restricted. The documentation states that you have a certain amount of API calls & responses each day. You can see your used points in the response header. Keep an eye on it when developing your script. \n",
    "- To avoid repeated calls to an API, use another cell in your notebook, once you stored the correct response information in a variable. This is one situation in which jupyter notebooks are very convenient.\n",
    "- Keep the .csv file, we might want to work with it in the future, but don't worry if you didn't manage, we'll provide you with a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to retrieve the information you are looking for, you need to add a query to the request url, \n",
    "# which is described in the API documentation\n",
    "\n",
    "import requests\n",
    "import spoonacular\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_batch1 = \"https://api.spoonacular.com/recipes/random?number=100&apiKey=d5f28412151545ddb2db42c4e5495546\"\n",
    "# endpoint_batch2 = \"https://api.spoonacular.com/recipes/random?number=50&apiKey=d5f28412151545ddb2db42c4e5495546\"\n",
    "\n",
    "r = requests.get(endpoint_batch1)\n",
    "results = r.json()\n",
    "\n",
    "filename = \"spoonacular_receipes_150_knh.csv\"\n",
    "header = ['vegetarian', 'glutenFree', 'dairyFree', 'veryHealthy', 'healthScore', 'aggregateLikes', 'id', 'title', 'pricePerServing','readyInMinutes','servings','sourceUrl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"spoonacular_receipes_150_knh.csv\", 'w') as f:\n",
    "    receipe_data = results['recipes']\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    for single_receipe in receipe_data:\n",
    "        attr_list = []\n",
    "        # Writing data of CSV file\n",
    "        attr_list.append(single_receipe['vegetarian'])\n",
    "        attr_list.append(single_receipe['glutenFree'])\n",
    "        attr_list.append(single_receipe['dairyFree'])\n",
    "        attr_list.append(single_receipe['veryHealthy'])\n",
    "        attr_list.append(single_receipe['healthScore'])\n",
    "        attr_list.append(single_receipe['aggregateLikes'])\n",
    "        attr_list.append(single_receipe['id'])\n",
    "        attr_list.append(single_receipe['title'])\n",
    "        attr_list.append(single_receipe['pricePerServing'])\n",
    "        attr_list.append(single_receipe['readyInMinutes'])\n",
    "        attr_list.append(single_receipe['servings'])\n",
    "        attr_list.append(single_receipe['sourceUrl'])\n",
    "        csv_writer.writerow(attr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spoonacular_receipes_150_knh.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"Please be aware, that the use of this API is restricted. \n",
    "The documentation states that you have a certain amount of API calls \n",
    "& responses each day. You can see your used points in the response header. \n",
    "Keep an eye on it when developing your script.\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional task: Web Scraping\n",
    "We want to create a list of all currently open data analytics jobs in Copenhagen:\n",
    "- job title\n",
    "- company\n",
    "- link to the job advert\n",
    "- publication data\n",
    "\n",
    "Since we cannot find an API or any other open dataset, we decide to scrape the publicly available homepage https://www.jobindex.dk/\n",
    "\n",
    "The information is available here: https://www.jobindex.dk/jobsoegning/storkoebenhavn?q=%27Data+Analyst%27\n",
    "\n",
    "Handin:\n",
    "**Handin:**\n",
    "- your notebook (.ipynb file)\n",
    "- your .csv file\n",
    "\n",
    "or\n",
    "\n",
    "- link to GitHub folder containing all the homework week 2 files (including the API call - files)\n",
    "\n",
    "**Hints** \n",
    "- Make sure to make use of the developer tools of your browser to inspect the website you are scraping and find the tags you need.\n",
    "- Make your scraper robust, so that it does not crash in case of unexpectet values in a record.\n",
    "- Be mindful of not overloading the website with calls, you might get blocked. You can use  `from time import sleep` and integrate the following line into your script for a 3 second break in repeated requests to the website: `sleep(3)`\n",
    "- The results are not all displayed on one page. Scrape the results from the first page first and then figure out how to get the results from the subsequent pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
